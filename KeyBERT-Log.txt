LOGGED ON: 2023-12-11 14:58:36.592370
METADATA: ['magicoder', 'opensource', 'llms', 'paperswithcode', 'chatgpt']
settings: keyphrase_ngram_range (1,1)  Diversity 0.32
---
ORIGINAL TEXT:

Title: Magicoder: Source Code Is All You Need
published on PaperswithCode
Authors: Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang
url: https://arxiv.org/pdf/2312.02120v1.pdf
We introduce Magicoder, a series of fully open-source (code, weights, and data)
Large Language Models (LLMs) for code that significantly closes the gap with top
code models while having no more than 7B parameters. Magicoder models are
trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach
to enlightening LLMs with open-source code snippets to generate high-quality
instruction data for code. Our main motivation is to mitigate the inherent bias of
the synthetic data generated by LLMs by empowering them with a wealth of opensource references for the production of more diverse, realistic, and controllable
data. The orthogonality of OSS-INSTRUCT and other data generation methods
like Evol-Instruct further enables us to build an enhanced MagicoderS. Both
Magicoder and MagicoderS substantially outperform state-of-the-art code models
with similar or even larger sizes on a wide range of coding benchmarks, including
Python text-to-code generation, multilingual coding, and data-science program
completion. Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses
the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall,
OSS-INSTRUCT opens a new direction for low-bias and high-quality instruction
tuning using abundant open-source references.

---


LOGGED ON: 2023-12-11 16:01:46.847419
METADATA: ['magicoders', 'opensource', 'chatgpt', 'instruct', 'llms']
settings: keyphrase_ngram_range (1,1)  Diversity 0.34
---
ORIGINAL TEXT:
We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of opensource references for the production of more diverse, realistic, and controllable data. The orthogonality of OSS-INSTRUCT and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion. Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall, OSS-INSTRUCT opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references. You are working on a natural language processing (NLP) project and need to create a program to preprocess and classify movie reviews... ... Your program should be able to preprocess new movie reviews, train the model, and classify new reviews accurately. Generated problem (details omitted) DPosNeg.py D
---


LOGGED ON: 2023-12-11 16:01:47.734101
METADATA: ['clean_review', 'learn_model', 'tf_idfsvm', 'sklearn', 'generated']
settings: keyphrase_ngram_range (1,1)  Diversity 0.34
---
ORIGINAL TEXT:
 (details omitted) DPosNeg.py DLog.cpp DGrantInfo.ts DProgram.cs DStrength.swift D… Open-source codebase learn_model( tf_idfSVM, tf_idfNB, target) def get_clean_review(raw_review): letters_only = re.sub( "[^a-zA-Z]", " ", raw_review) Seed code snippet OSS-INSTRUCT Please gain inspiration from the code snippet to create a highquality programming problem… Prompt (details omitted) from sklearn.feature_extraction.text import TfidfVectorizer ... def get_clean_review(raw_review): ... def train_model(tf_idfSVM, tf_idfNB, reviews, labels): ... def classify_review(clean_review, tf_idfSVM, tf_idfNB): ... ... train_model(tf_idfSVM, tf_idfNB, reviews, labels) cleaned_review = get_clean_review(...)... L Generated solution (details omitted) Language Model Figure 1: Overview of OSS-INSTRUCT and the pass@1 results of different LLMs on HumanEval (+) arXiv:2312.02120v1 [cs.CL] 4 Dec 2023 1 Introduction Code generation, also known as program synthesis [Gulwani et al., 2017], is a long-standing challenge in computer science. In the past few decades, a large body of research has been studying symbolic approaches, such as abstraction-based synthesis [Wang et al., 2017, Feng et al., 2018] for
---


LOGGED ON: 2023-12-11 16:01:48.580266
METADATA: ['generated', 'software', '2022', 'chatgpt', 'codellama']
settings: keyphrase_ngram_range (1,1)  Diversity 0.34
---
ORIGINAL TEXT:
., 2017, Feng et al., 2018] for general-purpose synthesis problems and programming by examples [Cambronero et al., 2023, Liu et al., 2023a] for domain-specific tasks. Until recently, Large Language Models (LLMs) trained on code [Austin et al., 2021, Chen et al., 2021] has shown outstanding breakthroughs in generating code that accurately satisfies user intents, and they are widely deployed to assist real-world software development [Microsoft, 2023b, Services, 2023]. Initially, closed-source models such as GPT-3.5 Turbo [OpenAI, 2022] (i.e., ChatGPT) and GPT4 [OpenAI, 2023] massively dominated various code generation benchmarks and leaderboards [Chen et al., 2021, Austin et al., 2021, Liu et al., 2023b, Lai et al., 2022]. To further push the boundaries of code generation with open source LLMs, SELF-INSTRUCT [Wang et al., 2023a] is adopted to bootstrap the instruction-following ability of LLMs. In the realm of code, practitioners commonly devise synthetic coding instructions using a stronger teacher model (e.g., ChatGPT and GPT-4) and then finetune a weaker student model (e.g., CODELLAMA [Rozière et al., 2023]) with the generated data to distill the knowledge from the teacher [Taori et al., 2023, Chaudhary, 2023].For example, Code Alpaca [Chaudhary, 2023] consists of 20K automatically generated code instructions by applying SELF-IN
---


LOGGED ON: 2023-12-11 16:01:49.374950
METADATA: ['instruct', 'heuristics', 'alpaca', 'llm', 'increase']
settings: keyphrase_ngram_range (1,1)  Diversity 0.34
---
ORIGINAL TEXT:
 automatically generated code instructions by applying SELF-INSTRUCT on ChatGPT using 21 seed tasks. To further enhance the coding abilities of LLMs, Luo et al. [2023b] proposes Code Evol-Instruct that employs various heuristics to increase the complexity of seed code instructions (Code Alpaca in this case), achieving state-of-the-art (SOTA) results among open-source models. While these data generation methods can effectively improve the instruction-following capability of an LLM, they rely on a narrow range of predefined tasks or heuristics under the hood.For example, on the one hand, Code Alpaca that adopts SELF-INSTRUCT only relies on 21 seed tasks to generate new code instructions using an identical prompt template. On the other hand, Code Evol-Instruct takes Code Alpaca as seeds and merely depends on 5 heuristics to evolve the dataset. As partly suggested by Yu et al. [2023] and [Wang et al., 2023a], such approaches may significantly inherit the system bias inherent in the LLMs as well as the predefined tasks. Therefore, in this paper, we propose OSS-INSTRUCT to mitigate the inherent bias of LLMs and to unleash their potential to craft high-quality and creative code instructions via direct learning from the open source. As shown in Figure 1, OSS-INSTRUCT leverages a powerful LLM to automatically generate new coding problems by drawing inspiration from any random code snippets collected from the open source. In this example, the LLM gets inspired by two incomplete code fragments from different functions and manages to relate them and craft a realistic machine learning problem. Thanks to the “
---


LOGGED ON: 2023-12-11 16:01:50.181301
METADATA: ['magicodercl', 'evalplus', 'instruct', 'mbpp', 'coding']
settings: keyphrase_ngram_range (1,1)  Diversity 0.34
---
ORIGINAL TEXT:
 realistic machine learning problem. Thanks to the “infinite” real-world open-source code, OSS-INSTRUCT can directly produce diverse, realistic, and controllable code instructions by providing distinct seed code snippets. In the end, we generate 75K synthetic data to finetune CODELLAMA-PYTHON-7B, resulting in Magicoder-CL. While being simple and effective, OSS-INSTRUCT is orthogonal to existing data generation methods, and they can be combined to further push the boundaries of the models’ coding capabilities. Therefore, we continually finetune Magicoder-CL on an open-source Evol-Instruct with 110K entries, producing MagicoderS-CL. We evaluate Magicoder and MagicoderS on a wide range of coding tasks, including HumanEval [Chen et al., 2021] and MBPP [Austin et al., 2021] for Python text-to-code generation, MultiPL-E [Cassano et al., 2022] for multilingual code completion, and DS-1000 [Lai et al., 2022] for solving data science problems. We further adopt EvalPlus [Liu et al., 2023b], which includes the augmented HumanEval+ and MBPP+ datasets for more rigorous model evaluation. Both MagicoderCL and MagicoderS-CL substantially boost the base CODELLAMA-PYTHON-7B. Additionally, Magicoder-CL even outperforms WizardCoder-CL-7B, WizardCoder-SC-15B, and all studied SOTA LLMs with less than or equal to 16B parameters on all the benchmarks we tested.Also, the pass@1
---


LOGGED ON: 2023-12-11 16:01:50.877203
METADATA: ['chatgpt', 'magicoders', 'deepseek', 'coding', 'oss']
settings: keyphrase_ngram_range (1,1)  Diversity 0.34
---
ORIGINAL TEXT:
 benchmarks we tested.Also, the pass@1 result of the enhanced MagicoderS-CL is on par with ChatGPT on HumanEval (70.7 vs. 72.6) and surpasses it on the more rigorous HumanEval+ (66.5 vs. 65.9), indicating that MagicoderS-CL can generate more robust code. It also achieves SOTA results among all code models at the same scale. Additionally, we notice a very recent advancement in the development of the DeepSeek-Coder series [DeepSeek AI, 2023] which has shown exceptional coding performance. However, due to the 2 You are exceptionally skilled at crafting high-quality programming problems and offering precise solutions. Please gain inspiration from the following random code snippet to create a high-quality programming problem. Present your output in two distinct sections: [Problem Description] and [Solution]. Code snippet for inspiration: ``` {code} ``` Guidelines for each section: 1. [Problem Description]: This should be **completely self-contained**, providing all the contextual information one needs to understand and solve the problem. Assume common programming knowledge, but ensure that any specific context, variables, or code snippets pertinent to this problem are explicitly included. 2. [Solution]: Offer a comprehensive, **correct** solution that accurately addresses the [Problem Description] you provided. Figure 2: The detailed prompt design for OSS-INSTRUCT limited technical details currently disclosed, we only briefly discuss them in §4.4. Despite this, we applied OSS-INSTRUCT on DeepSeek-Coder-Base 6.7B, resulting in the creation of Magicoder-DS and MagicoderS-DS. In
---


LOGGED ON: 2023-12-11 16:01:51.584120
METADATA: ['magicoders', 'coderinstruct', 'instruction', 'boost', 'llms']
settings: keyphrase_ngram_range (1,1)  Diversity 0.34
---
ORIGINAL TEXT:
-DS and MagicoderS-DS. In addition to the consistent findings on the previous results with CODELLAMAPYTHON-7B as the base model, Magicoder-DS and MagicoderS-DS benefit from the more powerful DeepSeek-Coder-Base-6.7B. This advantage is demonstrated by MagicoderS-DS, which achieves a remarkable 76.8 pass@1 on HumanEval. MagicoderS-DS also outperforms DeepSeek-CoderInstruct-6.7B on HumanEval, HumanEval+, MBPP, and MBPP+ with 8× less finetuning tokens. To justify the design of OSS-INSTRUCT, i.e., generating instruction-tuning data from open-source references rather than using the reference directly, we further demonstrate that finetuning the base models with semantically relevant comment-function pairs directly extracted from open-source projects even negatively impacts the model performance (§5.2). In general, we make the following contributions: • We introduce OSS-INSTRUCT, a pioneering approach to enlightening LLMs with open-source code snippets to generate more diverse, realistic, and controllable coding instruction data, which can be leveraged to substantially boost the performance of various LLMs via instruction tuning. It opens a new dimension for creating low-bias and high-quality instruction-tuning data from the abundance of open-source references. • We build the Magicoder series trained with OSS-INSTRUCT and MagicoderS series trained on a combination of OSS-INSTRUCT and Evol-Instruct. Our evaluation across 6 benchmarks shows that all Magicoders significantly improve the base LL
---


LOGGED ON: 2023-12-11 16:01:51.957885
METADATA: ['magicoders', 'chatgpt', 'llms', 'uiuc', 'humaneval']
settings: keyphrase_ngram_range (1,1)  Diversity 0.34
---
ORIGINAL TEXT:
 that all Magicoders significantly improve the base LLMs. Notably, both MagicoderS-CL and MagicoderS-DS outperform ChatGPT on HumanEval+ with only 7B parameters. • We fully open source the model weights, training data, and source code at https://github.com/ ise-uiuc/magicoder to facilitate future research.
---


