{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.8.3.tar.gz (29 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers in /Users/sohom/cuda/lib/python3.11/site-packages (4.34.1)\n",
      "Requirement already satisfied: langchain in /Users/sohom/cuda/lib/python3.11/site-packages (0.0.321)\n",
      "Requirement already satisfied: tiktoken in /Users/sohom/cuda/lib/python3.11/site-packages (0.5.1)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in /Users/sohom/cuda/lib/python3.11/site-packages (from keybert) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /Users/sohom/cuda/lib/python3.11/site-packages (from keybert) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/sohom/cuda/lib/python3.11/site-packages (from keybert) (1.25.2)\n",
      "Collecting rich>=10.4.0 (from keybert)\n",
      "  Obtaining dependency information for rich>=10.4.0 from https://files.pythonhosted.org/packages/be/be/1520178fa01eabe014b16e72a952b9f900631142ccd03dc36cf93e30c1ce/rich-13.7.0-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /Users/sohom/cuda/lib/python3.11/site-packages (from transformers) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/sohom/cuda/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sohom/cuda/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/sohom/cuda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/sohom/cuda/lib/python3.11/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sohom/cuda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain) (0.0.54)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/sohom/cuda/lib/python3.11/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sohom/cuda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sohom/cuda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sohom/cuda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.4.0->keybert)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from rich>=10.4.0->keybert) (2.16.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from scikit-learn>=0.22.2->keybert) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from scikit-learn>=0.22.2->keybert) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from scikit-learn>=0.22.2->keybert) (3.2.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from sentence-transformers>=0.3.8->keybert) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Users/sohom/cuda/lib/python3.11/site-packages (from sentence-transformers>=0.3.8->keybert) (0.16.0)\n",
      "Requirement already satisfied: nltk in /Users/sohom/cuda/lib/python3.11/site-packages (from sentence-transformers>=0.3.8->keybert) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/sohom/cuda/lib/python3.11/site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.99)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: sympy in /Users/sohom/cuda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/sohom/cuda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sohom/cuda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: click in /Users/sohom/cuda/lib/python3.11/site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.7)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/sohom/cuda/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
      "Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Building wheels for collected packages: keybert\n",
      "  Building wheel for keybert (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keybert: filename=keybert-0.8.3-py3-none-any.whl size=39124 sha256=3475ef300f6f9d62d1f31355fd0face50fa873f9e914e6da9d76cf5cbb5d8d17\n",
      "  Stored in directory: /Users/sohom/Library/Caches/pip/wheels/f7/0a/07/9dd07e4891a915b2f6311a73e0159e2858b27ec96e427e2b72\n",
      "Successfully built keybert\n",
      "Installing collected packages: mdurl, markdown-it-py, huggingface-hub, rich, keybert\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.19.4\n",
      "    Uninstalling huggingface-hub-0.19.4:\n",
      "      Successfully uninstalled huggingface-hub-0.19.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "diffusers 0.24.0 requires huggingface-hub>=0.19.4, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.17.3 keybert-0.8.3 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install keybert transformers langchain tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.rich import trange, tqdm\n",
    "from rich import console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "from rich.text import Text\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import datetime\n",
    "from rich.console import Console\n",
    "console = Console(width=110)\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc839cdc9934203a103da0336f357ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295387761dbb4b809b02014bb0458e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277c92e8940740c281c988506568a81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/179k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f44b7a25b5406fb14cee41edec695e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed382835ad2c4695a4ba60fc838e6b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09bcd3dedcd45bca17c3068aa27aa03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading onnx/config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7dae4be3e044e6ca958ae585fe59b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.onnx:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db73eb7c6c214d6b9af764b04a9b7973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56915408c45d48299b5aafa90dd3447a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd34f57ce62f449ea09cb5dd546d4f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353e41da03e946eb9db5fbdb860e449b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c213cb6f1243c4ba4400b2beb60854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2eec12b92f14d85b2f3dd4c5f3820b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79dd4b73c2a43988b63716920226c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afad35e6ee44a35aeda2b00565119cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d297bca31cba44b1b97d58fe5c7bfb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d18fcff3eba411c8b5757e1a978cd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e19067046046d6961587c0d2206435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT(model='intfloat/multilingual-e5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = 'KeyBERT-Log.txt'\n",
    "\n",
    "def writehistory(text):\n",
    "    with open(logfile, 'a', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keys(text, ngram,dvsity):\n",
    "    import datetime\n",
    "    import random\n",
    "    a = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, ngram), stop_words='english',\n",
    "                              use_mmr=True, diversity=dvsity, highlight=True)     #highlight=True\n",
    "    tags = []\n",
    "    for kw in a:\n",
    "        tags.append(str(kw[0]))\n",
    "    timestamped = datetime.datetime.now()\n",
    "    #LOG THE TEXT AND THE METATAGS\n",
    "    logging_text = f\"LOGGED ON: {str(timestamped)}\\nMETADATA: {str(tags)}\\nsettings: keyphrase_ngram_range (1,{str(ngram)})  Diversity {str(dvsity)}\\n---\\nORIGINAL TEXT:\\n{text}\\n---\\n\\n\"\n",
    "    writehistory(logging_text)\n",
    "    return tags\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Title: Magicoder: Source Code Is All You Need\n",
    "published on PaperswithCode\n",
    "Authors: Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang\n",
    "url: https://arxiv.org/pdf/2312.02120v1.pdf\n",
    "We introduce Magicoder, a series of fully open-source (code, weights, and data)\n",
    "Large Language Models (LLMs) for code that significantly closes the gap with top\n",
    "code models while having no more than 7B parameters. Magicoder models are\n",
    "trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach\n",
    "to enlightening LLMs with open-source code snippets to generate high-quality\n",
    "instruction data for code. Our main motivation is to mitigate the inherent bias of\n",
    "the synthetic data generated by LLMs by empowering them with a wealth of opensource references for the production of more diverse, realistic, and controllable\n",
    "data. The orthogonality of OSS-INSTRUCT and other data generation methods\n",
    "like Evol-Instruct further enables us to build an enhanced MagicoderS. Both\n",
    "Magicoder and MagicoderS substantially outperform state-of-the-art code models\n",
    "with similar or even larger sizes on a wide range of coding benchmarks, including\n",
    "Python text-to-code generation, multilingual coding, and data-science program\n",
    "completion. Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses\n",
    "the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall,\n",
    "OSS-INSTRUCT opens a new direction for low-bias and high-quality instruction\n",
    "tuning using abundant open-source references.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Title <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Source Code Is All You Need published on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">PaperswithCode</span> Authors Yuxiang Wei Zhe Wang Jiawei Liu \n",
       "Yifeng Ding Lingming Zhang url https arxiv org pdf 2312 02120v1 pdf We introduce <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> series of fully open \n",
       "source code weights and data Large Language Models <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> for code that significantly closes the gap with top code \n",
       "models while having no more than 7B parameters <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> models are trained on 75K synthetic instruction data using\n",
       "OSS INSTRUCT novel approach to enlightening <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> with open source code snippets to generate high quality \n",
       "instruction data for code Our main motivation is to mitigate the inherent bias of the synthetic data generated by \n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> by empowering them with wealth of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">opensource</span> references for the production of more diverse realistic and \n",
       "controllable data The orthogonality of OSS INSTRUCT and other data generation methods like Evol Instruct further \n",
       "enables us to build an enhanced MagicoderS Both <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> and MagicoderS substantially outperform state of the art \n",
       "code models with similar or even larger sizes on wide range of coding benchmarks including Python text to code \n",
       "generation multilingual coding and data science program completion Notably MagicoderS CL 7B based on CODELLAMA even\n",
       "surpasses the prominent <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> on HumanEval 66 vs 65 in pass Overall OSS INSTRUCT opens new direction for low bias\n",
       "and high quality instruction tuning using abundant open source references\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Title \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Source Code Is All You Need published on \u001b[30;48;2;255;255;0mPaperswithCode\u001b[0m Authors Yuxiang Wei Zhe Wang Jiawei Liu \n",
       "Yifeng Ding Lingming Zhang url https arxiv org pdf 2312 02120v1 pdf We introduce \u001b[30;48;2;255;255;0mMagicoder\u001b[0m series of fully open \n",
       "source code weights and data Large Language Models \u001b[30;48;2;255;255;0mLLMs\u001b[0m for code that significantly closes the gap with top code \n",
       "models while having no more than 7B parameters \u001b[30;48;2;255;255;0mMagicoder\u001b[0m models are trained on 75K synthetic instruction data using\n",
       "OSS INSTRUCT novel approach to enlightening \u001b[30;48;2;255;255;0mLLMs\u001b[0m with open source code snippets to generate high quality \n",
       "instruction data for code Our main motivation is to mitigate the inherent bias of the synthetic data generated by \n",
       "\u001b[30;48;2;255;255;0mLLMs\u001b[0m by empowering them with wealth of \u001b[30;48;2;255;255;0mopensource\u001b[0m references for the production of more diverse realistic and \n",
       "controllable data The orthogonality of OSS INSTRUCT and other data generation methods like Evol Instruct further \n",
       "enables us to build an enhanced MagicoderS Both \u001b[30;48;2;255;255;0mMagicoder\u001b[0m and MagicoderS substantially outperform state of the art \n",
       "code models with similar or even larger sizes on wide range of coding benchmarks including Python text to code \n",
       "generation multilingual coding and data science program completion Notably MagicoderS CL 7B based on CODELLAMA even\n",
       "surpasses the prominent \u001b[30;48;2;255;255;0mChatGPT\u001b[0m on HumanEval 66 vs 65 in pass Overall OSS INSTRUCT opens new direction for low bias\n",
       "and high quality instruction tuning using abundant open source references\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Keywords: [</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'magicoder'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'opensource'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'llms'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'paperswithcode'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'chatgpt'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mKeywords: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;32m'magicoder'\u001b[0m\u001b[1m, \u001b[0m\u001b[1;32m'opensource'\u001b[0m\u001b[1m, \u001b[0m\u001b[1;32m'llms'\u001b[0m\u001b[1m, \u001b[0m\u001b[1;32m'paperswithcode'\u001b[0m\u001b[1m, \u001b[0m\u001b[1;32m'chatgpt'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = extract_keys(text, 1,0.32)\n",
    "console.print(f\"[bold]Keywords: {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Text has been saved into variable <span style=\"font-weight: bold\">fulltext</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Text has been saved into variable \u001b[1mfulltext\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = 'Magicoder Source Code Is All You Need.txt'\n",
    "with open(filename, encoding=\"utf8\") as f:\n",
    "  fulltext = f.read()\n",
    "f.close()\n",
    "console.print(\"Text has been saved into variable [bold]fulltext\")\n",
    "title = 'Magicoder: Source Code Is All You Need'\n",
    "filename = '2023-12-03 18.41.12 Governing societies with Artificial Intelligence.txt'\n",
    "author = 'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'\n",
    "url = 'https://arxiv.org/pdf/2312.02120v1.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=350, chunk_overlap=10)\n",
    "splitted_text = text_splitter.split_text(fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m8\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">We introduce Magicoder, a series of fully open-source <span style=\"font-weight: bold\">(</span>code, weights, and data<span style=\"font-weight: bold\">)</span> Large Language Models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> \n",
       "for code that significantly closes the gap with top code models while having no more than 7B parameters. \n",
       "Magicoder models are trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach to \n",
       "enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. Our main \n",
       "motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a \n",
       "wealth of opensource references for the production of more diverse, realistic, and controllable data. The \n",
       "orthogonality of OSS-INSTRUCT and other data generation methods like Evol-Instruct further enables us to build\n",
       "an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models \n",
       "with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code \n",
       "generation, multilingual coding, and data-science program completion. Notably, MagicoderS-CL-7B based on \n",
       "CODELLAMA even surpasses the prominent ChatGPT on HumanEval+ <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66.5</span> vs. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65.9</span> in pass@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>. Overall, OSS-INSTRUCT \n",
       "opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references. \n",
       "You are working on a natural language processing <span style=\"font-weight: bold\">(</span>NLP<span style=\"font-weight: bold\">)</span> project and need to create a program to preprocess and \n",
       "classify movie reviews<span style=\"color: #808000; text-decoration-color: #808000\">...</span> <span style=\"color: #808000; text-decoration-color: #808000\">...</span> Your program should be able to preprocess new movie reviews, train the model, \n",
       "and classify new reviews accurately. Generated problem <span style=\"font-weight: bold\">(</span>details omitted<span style=\"font-weight: bold\">)</span> DPosNeg.py D\n",
       "</pre>\n"
      ],
      "text/plain": [
       "We introduce Magicoder, a series of fully open-source \u001b[1m(\u001b[0mcode, weights, and data\u001b[1m)\u001b[0m Large Language Models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m \n",
       "for code that significantly closes the gap with top code models while having no more than 7B parameters. \n",
       "Magicoder models are trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach to \n",
       "enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. Our main \n",
       "motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a \n",
       "wealth of opensource references for the production of more diverse, realistic, and controllable data. The \n",
       "orthogonality of OSS-INSTRUCT and other data generation methods like Evol-Instruct further enables us to build\n",
       "an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models \n",
       "with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code \n",
       "generation, multilingual coding, and data-science program completion. Notably, MagicoderS-CL-7B based on \n",
       "CODELLAMA even surpasses the prominent ChatGPT on HumanEval+ \u001b[1m(\u001b[0m\u001b[1;36m66.5\u001b[0m vs. \u001b[1;36m65.9\u001b[0m in pass@\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m. Overall, OSS-INSTRUCT \n",
       "opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references. \n",
       "You are working on a natural language processing \u001b[1m(\u001b[0mNLP\u001b[1m)\u001b[0m project and need to create a program to preprocess and \n",
       "classify movie reviews\u001b[33m...\u001b[0m \u001b[33m...\u001b[0m Your program should be able to preprocess new movie reviews, train the model, \n",
       "and classify new reviews accurately. Generated problem \u001b[1m(\u001b[0mdetails omitted\u001b[1m)\u001b[0m DPosNeg.py D\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(len(splitted_text))\n",
    "console.print(\"---\")\n",
    "console.print(splitted_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b74be5bbf2d4d91b80c063ae370a1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">We introduce Magicoder series of fully open source code weights and data Large Language Models <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> for code that \n",
       "significantly closes the gap with top code models while having no more than 7B parameters Magicoder models are \n",
       "trained on 75K synthetic instruction data using OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> novel approach to enlightening <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> with open source \n",
       "code snippets to generate high quality instruction data for code Our main motivation is to mitigate the inherent \n",
       "bias of the synthetic data generated by <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> by empowering them with wealth of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">opensource</span> references for the \n",
       "production of more diverse realistic and controllable data The orthogonality of OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> and other data \n",
       "generation methods like Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> further enables us to build an enhanced <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> Both Magicoder and \n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> substantially outperform state of the art code models with similar or even larger sizes on wide range of\n",
       "coding benchmarks including Python text to code generation multilingual coding and data science program completion \n",
       "Notably <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL 7B based on CODELLAMA even surpasses the prominent <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> on HumanEval 66 vs 65 in pass \n",
       "Overall OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> opens new direction for low bias and high quality instruction tuning using abundant open \n",
       "source references You are working on natural language processing NLP project and need to create program to \n",
       "preprocess and classify movie reviews Your program should be able to preprocess new movie reviews train the model \n",
       "and classify new reviews accurately Generated problem details omitted DPosNeg py\n",
       "</pre>\n"
      ],
      "text/plain": [
       "We introduce Magicoder series of fully open source code weights and data Large Language Models \u001b[30;48;2;255;255;0mLLMs\u001b[0m for code that \n",
       "significantly closes the gap with top code models while having no more than 7B parameters Magicoder models are \n",
       "trained on 75K synthetic instruction data using OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m novel approach to enlightening \u001b[30;48;2;255;255;0mLLMs\u001b[0m with open source \n",
       "code snippets to generate high quality instruction data for code Our main motivation is to mitigate the inherent \n",
       "bias of the synthetic data generated by \u001b[30;48;2;255;255;0mLLMs\u001b[0m by empowering them with wealth of \u001b[30;48;2;255;255;0mopensource\u001b[0m references for the \n",
       "production of more diverse realistic and controllable data The orthogonality of OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m and other data \n",
       "generation methods like Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m further enables us to build an enhanced \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m Both Magicoder and \n",
       "\u001b[30;48;2;255;255;0mMagicoderS\u001b[0m substantially outperform state of the art code models with similar or even larger sizes on wide range of\n",
       "coding benchmarks including Python text to code generation multilingual coding and data science program completion \n",
       "Notably \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL 7B based on CODELLAMA even surpasses the prominent \u001b[30;48;2;255;255;0mChatGPT\u001b[0m on HumanEval 66 vs 65 in pass \n",
       "Overall OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m opens new direction for low bias and high quality instruction tuning using abundant open \n",
       "source references You are working on natural language processing NLP project and need to create program to \n",
       "preprocess and classify movie reviews Your program should be able to preprocess new movie reviews train the model \n",
       "and classify new reviews accurately Generated problem details omitted DPosNeg py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">details omitted DPosNeg py DLog cpp DGrantInfo ts DProgram cs DStrength swift Open source codebase <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">learn_model</span> \n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tf_idfSVM</span> tf_idfNB target def get_clean_review raw_review letters_only re sub zA raw_review Seed code snippet OSS \n",
       "INSTRUCT Please gain inspiration from the code snippet to create highquality programming problem Prompt details \n",
       "omitted from <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">sklearn</span> feature_extraction text import TfidfVectorizer def get_clean_review raw_review def train_model\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tf_idfSVM</span> tf_idfNB reviews labels def classify_review <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">clean_review</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tf_idfSVM</span> tf_idfNB train_model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tf_idfSVM</span> \n",
       "tf_idfNB reviews labels cleaned_review get_clean_review <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Generated</span> solution details omitted Language Model Figure \n",
       "Overview of OSS INSTRUCT and the pass results of different LLMs on HumanEval arXiv 2312 02120v1 cs CL Dec 2023 \n",
       "Introduction Code generation also known as program synthesis Gulwani et al 2017 is long standing challenge in \n",
       "computer science In the past few decades large body of research has been studying symbolic approaches such as \n",
       "abstraction based synthesis Wang et al 2017 Feng et al 2018 for\n",
       "</pre>\n"
      ],
      "text/plain": [
       "details omitted DPosNeg py DLog cpp DGrantInfo ts DProgram cs DStrength swift Open source codebase \u001b[30;48;2;255;255;0mlearn_model\u001b[0m \n",
       "\u001b[30;48;2;255;255;0mtf_idfSVM\u001b[0m tf_idfNB target def get_clean_review raw_review letters_only re sub zA raw_review Seed code snippet OSS \n",
       "INSTRUCT Please gain inspiration from the code snippet to create highquality programming problem Prompt details \n",
       "omitted from \u001b[30;48;2;255;255;0msklearn\u001b[0m feature_extraction text import TfidfVectorizer def get_clean_review raw_review def train_model\n",
       "\u001b[30;48;2;255;255;0mtf_idfSVM\u001b[0m tf_idfNB reviews labels def classify_review \u001b[30;48;2;255;255;0mclean_review\u001b[0m \u001b[30;48;2;255;255;0mtf_idfSVM\u001b[0m tf_idfNB train_model \u001b[30;48;2;255;255;0mtf_idfSVM\u001b[0m \n",
       "tf_idfNB reviews labels cleaned_review get_clean_review \u001b[30;48;2;255;255;0mGenerated\u001b[0m solution details omitted Language Model Figure \n",
       "Overview of OSS INSTRUCT and the pass results of different LLMs on HumanEval arXiv 2312 02120v1 cs CL Dec 2023 \n",
       "Introduction Code generation also known as program synthesis Gulwani et al 2017 is long standing challenge in \n",
       "computer science In the past few decades large body of research has been studying symbolic approaches such as \n",
       "abstraction based synthesis Wang et al 2017 Feng et al 2018 for\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2017 Feng et al 2018 for general purpose synthesis problems and programming by examples Cambronero et al 2023 Liu \n",
       "et al 2023a for domain specific tasks Until recently Large Language Models LLMs trained on code Austin et al 2021 \n",
       "Chen et al 2021 has shown outstanding breakthroughs in generating code that accurately satisfies user intents and \n",
       "they are widely deployed to assist real world <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">software</span> development Microsoft 2023b Services 2023 Initially closed \n",
       "source models such as GPT Turbo OpenAI <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> and GPT4 OpenAI 2023 massively dominated various code \n",
       "generation benchmarks and leaderboards Chen et al 2021 Austin et al 2021 Liu et al 2023b Lai et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> To further \n",
       "push the boundaries of code generation with open source LLMs SELF INSTRUCT Wang et al 2023a is adopted to bootstrap\n",
       "the instruction following ability of LLMs In the realm of code practitioners commonly devise synthetic coding \n",
       "instructions using stronger teacher model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> and GPT and then finetune weaker student model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> Rozière \n",
       "et al 2023 with the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">generated</span> data to distill the knowledge from the teacher Taori et al 2023 Chaudhary 2023 For \n",
       "example Code Alpaca Chaudhary 2023 consists of 20K automatically <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">generated</span> code instructions by applying SELF IN\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2017 Feng et al 2018 for general purpose synthesis problems and programming by examples Cambronero et al 2023 Liu \n",
       "et al 2023a for domain specific tasks Until recently Large Language Models LLMs trained on code Austin et al 2021 \n",
       "Chen et al 2021 has shown outstanding breakthroughs in generating code that accurately satisfies user intents and \n",
       "they are widely deployed to assist real world \u001b[30;48;2;255;255;0msoftware\u001b[0m development Microsoft 2023b Services 2023 Initially closed \n",
       "source models such as GPT Turbo OpenAI \u001b[30;48;2;255;255;0m2022\u001b[0m \u001b[30;48;2;255;255;0mChatGPT\u001b[0m and GPT4 OpenAI 2023 massively dominated various code \n",
       "generation benchmarks and leaderboards Chen et al 2021 Austin et al 2021 Liu et al 2023b Lai et al \u001b[30;48;2;255;255;0m2022\u001b[0m To further \n",
       "push the boundaries of code generation with open source LLMs SELF INSTRUCT Wang et al 2023a is adopted to bootstrap\n",
       "the instruction following ability of LLMs In the realm of code practitioners commonly devise synthetic coding \n",
       "instructions using stronger teacher model \u001b[30;48;2;255;255;0mChatGPT\u001b[0m and GPT and then finetune weaker student model \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m Rozière \n",
       "et al 2023 with the \u001b[30;48;2;255;255;0mgenerated\u001b[0m data to distill the knowledge from the teacher Taori et al 2023 Chaudhary 2023 For \n",
       "example Code Alpaca Chaudhary 2023 consists of 20K automatically \u001b[30;48;2;255;255;0mgenerated\u001b[0m code instructions by applying SELF IN\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">automatically generated code instructions by applying SELF <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> on ChatGPT using 21 seed tasks To further \n",
       "enhance the coding abilities of LLMs Luo et al 2023b proposes Code Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> that employs various <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">heuristics</span> to\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">increase</span> the complexity of seed code instructions Code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Alpaca</span> in this case achieving state of the art SOTA results \n",
       "among open source models While these data generation methods can effectively improve the instruction following \n",
       "capability of an <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLM</span> they rely on narrow range of predefined tasks or <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">heuristics</span> under the hood For example on the \n",
       "one hand Code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Alpaca</span> that adopts SELF <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> only relies on 21 seed tasks to generate new code instructions using\n",
       "an identical prompt template On the other hand Code Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> takes Code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Alpaca</span> as seeds and merely depends on \n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">heuristics</span> to evolve the dataset As partly suggested by Yu et al 2023 and Wang et al 2023a such approaches may \n",
       "significantly inherit the system bias inherent in the LLMs as well as the predefined tasks Therefore in this paper \n",
       "we propose OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> to mitigate the inherent bias of LLMs and to unleash their potential to craft high quality \n",
       "and creative code instructions via direct learning from the open source As shown in Figure OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> leverages \n",
       "powerful <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLM</span> to automatically generate new coding problems by drawing inspiration from any random code snippets \n",
       "collected from the open source In this example the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLM</span> gets inspired by two incomplete code fragments from \n",
       "different functions and manages to relate them and craft realistic machine learning problem Thanks to the\n",
       "</pre>\n"
      ],
      "text/plain": [
       "automatically generated code instructions by applying SELF \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m on ChatGPT using 21 seed tasks To further \n",
       "enhance the coding abilities of LLMs Luo et al 2023b proposes Code Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m that employs various \u001b[30;48;2;255;255;0mheuristics\u001b[0m to\n",
       "\u001b[30;48;2;255;255;0mincrease\u001b[0m the complexity of seed code instructions Code \u001b[30;48;2;255;255;0mAlpaca\u001b[0m in this case achieving state of the art SOTA results \n",
       "among open source models While these data generation methods can effectively improve the instruction following \n",
       "capability of an \u001b[30;48;2;255;255;0mLLM\u001b[0m they rely on narrow range of predefined tasks or \u001b[30;48;2;255;255;0mheuristics\u001b[0m under the hood For example on the \n",
       "one hand Code \u001b[30;48;2;255;255;0mAlpaca\u001b[0m that adopts SELF \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m only relies on 21 seed tasks to generate new code instructions using\n",
       "an identical prompt template On the other hand Code Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m takes Code \u001b[30;48;2;255;255;0mAlpaca\u001b[0m as seeds and merely depends on \n",
       "\u001b[30;48;2;255;255;0mheuristics\u001b[0m to evolve the dataset As partly suggested by Yu et al 2023 and Wang et al 2023a such approaches may \n",
       "significantly inherit the system bias inherent in the LLMs as well as the predefined tasks Therefore in this paper \n",
       "we propose OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m to mitigate the inherent bias of LLMs and to unleash their potential to craft high quality \n",
       "and creative code instructions via direct learning from the open source As shown in Figure OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m leverages \n",
       "powerful \u001b[30;48;2;255;255;0mLLM\u001b[0m to automatically generate new coding problems by drawing inspiration from any random code snippets \n",
       "collected from the open source In this example the \u001b[30;48;2;255;255;0mLLM\u001b[0m gets inspired by two incomplete code fragments from \n",
       "different functions and manages to relate them and craft realistic machine learning problem Thanks to the\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">realistic machine learning problem Thanks to the infinite real world open source code OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> can directly \n",
       "produce diverse realistic and controllable code instructions by providing distinct seed code snippets In the end we\n",
       "generate 75K synthetic data to finetune CODELLAMA PYTHON 7B resulting in Magicoder CL While being simple and \n",
       "effective OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> is orthogonal to existing data generation methods and they can be combined to further push \n",
       "the boundaries of the models <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">coding</span> capabilities Therefore we continually finetune Magicoder CL on an open source \n",
       "Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> with 110K entries producing MagicoderS CL We evaluate Magicoder and MagicoderS on wide range of \n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">coding</span> tasks including HumanEval Chen et al 2021 and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span> Austin et al 2021 for Python text to code generation \n",
       "MultiPL Cassano et al 2022 for multilingual code completion and DS 1000 Lai et al 2022 for solving data science \n",
       "problems We further adopt <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">EvalPlus</span> Liu et al 2023b which includes the augmented HumanEval and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span> datasets for \n",
       "more rigorous model evaluation Both <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderCL</span> and MagicoderS CL substantially boost the base CODELLAMA PYTHON 7B \n",
       "Additionally Magicoder CL even outperforms WizardCoder CL 7B WizardCoder SC 15B and all studied SOTA LLMs with less\n",
       "than or equal to 16B parameters on all the benchmarks we tested Also the pass\n",
       "</pre>\n"
      ],
      "text/plain": [
       "realistic machine learning problem Thanks to the infinite real world open source code OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m can directly \n",
       "produce diverse realistic and controllable code instructions by providing distinct seed code snippets In the end we\n",
       "generate 75K synthetic data to finetune CODELLAMA PYTHON 7B resulting in Magicoder CL While being simple and \n",
       "effective OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m is orthogonal to existing data generation methods and they can be combined to further push \n",
       "the boundaries of the models \u001b[30;48;2;255;255;0mcoding\u001b[0m capabilities Therefore we continually finetune Magicoder CL on an open source \n",
       "Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m with 110K entries producing MagicoderS CL We evaluate Magicoder and MagicoderS on wide range of \n",
       "\u001b[30;48;2;255;255;0mcoding\u001b[0m tasks including HumanEval Chen et al 2021 and \u001b[30;48;2;255;255;0mMBPP\u001b[0m Austin et al 2021 for Python text to code generation \n",
       "MultiPL Cassano et al 2022 for multilingual code completion and DS 1000 Lai et al 2022 for solving data science \n",
       "problems We further adopt \u001b[30;48;2;255;255;0mEvalPlus\u001b[0m Liu et al 2023b which includes the augmented HumanEval and \u001b[30;48;2;255;255;0mMBPP\u001b[0m datasets for \n",
       "more rigorous model evaluation Both \u001b[30;48;2;255;255;0mMagicoderCL\u001b[0m and MagicoderS CL substantially boost the base CODELLAMA PYTHON 7B \n",
       "Additionally Magicoder CL even outperforms WizardCoder CL 7B WizardCoder SC 15B and all studied SOTA LLMs with less\n",
       "than or equal to 16B parameters on all the benchmarks we tested Also the pass\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">benchmarks we tested Also the pass result of the enhanced <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL is on par with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> on HumanEval 70 vs \n",
       "72 and surpasses it on the more rigorous HumanEval 66 vs 65 indicating that <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL can generate more robust \n",
       "code It also achieves SOTA results among all code models at the same scale Additionally we notice very recent \n",
       "advancement in the development of the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder series <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> AI 2023 which has shown exceptional <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">coding</span> \n",
       "performance However due to the You are exceptionally skilled at crafting high quality programming problems and \n",
       "offering precise solutions Please gain inspiration from the following random code snippet to create high quality \n",
       "programming problem Present your output in two distinct sections Problem Description and Solution Code snippet for \n",
       "inspiration code Guidelines for each section Problem Description This should be completely self contained providing\n",
       "all the contextual information one needs to understand and solve the problem Assume common programming knowledge \n",
       "but ensure that any specific context variables or code snippets pertinent to this problem are explicitly included \n",
       "Solution Offer comprehensive correct solution that accurately addresses the Problem Description you provided Figure\n",
       "The detailed prompt design for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT limited technical details currently disclosed we only briefly discuss \n",
       "them in Despite this we applied <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder Base 7B resulting in the creation of Magicoder DS \n",
       "and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> DS In\n",
       "</pre>\n"
      ],
      "text/plain": [
       "benchmarks we tested Also the pass result of the enhanced \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL is on par with \u001b[30;48;2;255;255;0mChatGPT\u001b[0m on HumanEval 70 vs \n",
       "72 and surpasses it on the more rigorous HumanEval 66 vs 65 indicating that \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL can generate more robust \n",
       "code It also achieves SOTA results among all code models at the same scale Additionally we notice very recent \n",
       "advancement in the development of the \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder series \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m AI 2023 which has shown exceptional \u001b[30;48;2;255;255;0mcoding\u001b[0m \n",
       "performance However due to the You are exceptionally skilled at crafting high quality programming problems and \n",
       "offering precise solutions Please gain inspiration from the following random code snippet to create high quality \n",
       "programming problem Present your output in two distinct sections Problem Description and Solution Code snippet for \n",
       "inspiration code Guidelines for each section Problem Description This should be completely self contained providing\n",
       "all the contextual information one needs to understand and solve the problem Assume common programming knowledge \n",
       "but ensure that any specific context variables or code snippets pertinent to this problem are explicitly included \n",
       "Solution Offer comprehensive correct solution that accurately addresses the Problem Description you provided Figure\n",
       "The detailed prompt design for \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT limited technical details currently disclosed we only briefly discuss \n",
       "them in Despite this we applied \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT on \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder Base 7B resulting in the creation of Magicoder DS \n",
       "and \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m DS In\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DS and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> DS In addition to the consistent findings on the previous results with CODELLAMAPYTHON 7B as the \n",
       "base model Magicoder DS and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> DS benefit from the more powerful DeepSeek Coder Base 7B This advantage is \n",
       "demonstrated by <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> DS which achieves remarkable 76 pass on HumanEval <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> DS also outperforms \n",
       "DeepSeek <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CoderInstruct</span> 7B on HumanEval HumanEval MBPP and MBPP with less finetuning tokens To justify the design of\n",
       "OSS INSTRUCT generating <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">instruction</span> tuning data from open source references rather than using the reference \n",
       "directly we further demonstrate that finetuning the base models with semantically relevant comment function pairs \n",
       "directly extracted from open source projects even negatively impacts the model performance In general we make the \n",
       "following contributions We introduce OSS INSTRUCT pioneering approach to enlightening <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> with open source code \n",
       "snippets to generate more diverse realistic and controllable coding <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">instruction</span> data which can be leveraged to \n",
       "substantially <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">boost</span> the performance of various <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> via <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">instruction</span> tuning It opens new dimension for creating low \n",
       "bias and high quality <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">instruction</span> tuning data from the abundance of open source references We build the Magicoder \n",
       "series trained with OSS INSTRUCT and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> series trained on combination of OSS INSTRUCT and Evol Instruct Our\n",
       "evaluation across benchmarks shows that all <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoders</span> significantly improve the base LL\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DS and \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m DS In addition to the consistent findings on the previous results with CODELLAMAPYTHON 7B as the \n",
       "base model Magicoder DS and \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m DS benefit from the more powerful DeepSeek Coder Base 7B This advantage is \n",
       "demonstrated by \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m DS which achieves remarkable 76 pass on HumanEval \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m DS also outperforms \n",
       "DeepSeek \u001b[30;48;2;255;255;0mCoderInstruct\u001b[0m 7B on HumanEval HumanEval MBPP and MBPP with less finetuning tokens To justify the design of\n",
       "OSS INSTRUCT generating \u001b[30;48;2;255;255;0minstruction\u001b[0m tuning data from open source references rather than using the reference \n",
       "directly we further demonstrate that finetuning the base models with semantically relevant comment function pairs \n",
       "directly extracted from open source projects even negatively impacts the model performance In general we make the \n",
       "following contributions We introduce OSS INSTRUCT pioneering approach to enlightening \u001b[30;48;2;255;255;0mLLMs\u001b[0m with open source code \n",
       "snippets to generate more diverse realistic and controllable coding \u001b[30;48;2;255;255;0minstruction\u001b[0m data which can be leveraged to \n",
       "substantially \u001b[30;48;2;255;255;0mboost\u001b[0m the performance of various \u001b[30;48;2;255;255;0mLLMs\u001b[0m via \u001b[30;48;2;255;255;0minstruction\u001b[0m tuning It opens new dimension for creating low \n",
       "bias and high quality \u001b[30;48;2;255;255;0minstruction\u001b[0m tuning data from the abundance of open source references We build the Magicoder \n",
       "series trained with OSS INSTRUCT and \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m series trained on combination of OSS INSTRUCT and Evol Instruct Our\n",
       "evaluation across benchmarks shows that all \u001b[30;48;2;255;255;0mMagicoders\u001b[0m significantly improve the base LL\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">that all <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoders</span> significantly improve the base <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> Notably both <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> DS outperform \n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">HumanEval</span> with only 7B parameters We fully open source the model weights training data and source code \n",
       "at https github com ise <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">uiuc</span> magicoder to facilitate future research\n",
       "</pre>\n"
      ],
      "text/plain": [
       "that all \u001b[30;48;2;255;255;0mMagicoders\u001b[0m significantly improve the base \u001b[30;48;2;255;255;0mLLMs\u001b[0m Notably both \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL and \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m DS outperform \n",
       "\u001b[30;48;2;255;255;0mChatGPT\u001b[0m on \u001b[30;48;2;255;255;0mHumanEval\u001b[0m with only 7B parameters We fully open source the model weights training data and source code \n",
       "at https github com ise \u001b[30;48;2;255;255;0muiuc\u001b[0m magicoder to facilitate future research\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys = []\n",
    "for i in trange(0,len(splitted_text)):\n",
    "  text = splitted_text[i]\n",
    "  keys.append({'document' : filename,\n",
    "              'title' : title,\n",
    "              'author' : author,\n",
    "              'url' : url,\n",
    "              'doc': text,\n",
    "              'keywords' : extract_keys(text, 1, 0.34)\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'document'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-03 18.41.12 Governing societies with Artificial Intelligence.txt'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Magicoder: Source Code Is All You Need'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://arxiv.org/pdf/2312.02120v1.pdf'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'doc'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">' (details omitted) DPosNeg.py DLog.cpp DGrantInfo.ts DProgram.cs DStrength.swift D… Open-source </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">codebase learn_model( tf_idfSVM, tf_idfNB, target) def get_clean_review(raw_review): letters_only = re.sub( </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"[^a-zA-Z]\", \" \", raw_review) Seed code snippet OSS-INSTRUCT Please gain inspiration from the code snippet to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">create a highquality programming problem… Prompt (details omitted) from sklearn.feature_extraction.text import</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">TfidfVectorizer ... def get_clean_review(raw_review): ... def train_model(tf_idfSVM, tf_idfNB, reviews, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">labels): ... def classify_review(clean_review, tf_idfSVM, tf_idfNB): ... ... train_model(tf_idfSVM, tf_idfNB, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reviews, labels) cleaned_review = get_clean_review(...)... L Generated solution (details omitted) Language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Model Figure 1: Overview of OSS-INSTRUCT and the pass@1 results of different LLMs on HumanEval (+) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">arXiv:2312.02120v1 [cs.CL] 4 Dec 2023 1 Introduction Code generation, also known as program synthesis [Gulwani</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al., 2017], is a long-standing challenge in computer science. In the past few decades, a large body of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">research has been studying symbolic approaches, such as abstraction-based synthesis [Wang et al., 2017, Feng </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al., 2018] for'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'clean_review'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'learn_model'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tf_idfsvm'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sklearn'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'generated'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'document'\u001b[0m: \u001b[32m'2023-12-03 18.41.12 Governing societies with Artificial Intelligence.txt'\u001b[0m,\n",
       "    \u001b[32m'title'\u001b[0m: \u001b[32m'Magicoder: Source Code Is All You Need'\u001b[0m,\n",
       "    \u001b[32m'author'\u001b[0m: \u001b[32m'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'\u001b[0m,\n",
       "    \u001b[32m'url'\u001b[0m: \u001b[32m'https://arxiv.org/pdf/2312.02120v1.pdf'\u001b[0m,\n",
       "    \u001b[32m'doc'\u001b[0m: \u001b[32m' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdetails omitted\u001b[0m\u001b[32m)\u001b[0m\u001b[32m DPosNeg.py DLog.cpp DGrantInfo.ts DProgram.cs DStrength.swift D… Open-source \u001b[0m\n",
       "\u001b[32mcodebase learn_model\u001b[0m\u001b[32m(\u001b[0m\u001b[32m tf_idfSVM, tf_idfNB, target\u001b[0m\u001b[32m)\u001b[0m\u001b[32m def get_clean_review\u001b[0m\u001b[32m(\u001b[0m\u001b[32mraw_review\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: letters_only = re.sub\u001b[0m\u001b[32m(\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32m^a-zA-Z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\", \" \", raw_review\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Seed code snippet OSS-INSTRUCT Please gain inspiration from the code snippet to \u001b[0m\n",
       "\u001b[32mcreate a highquality programming problem… Prompt \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdetails omitted\u001b[0m\u001b[32m)\u001b[0m\u001b[32m from sklearn.feature_extraction.text import\u001b[0m\n",
       "\u001b[32mTfidfVectorizer ... def get_clean_review\u001b[0m\u001b[32m(\u001b[0m\u001b[32mraw_review\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: ... def train_model\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtf_idfSVM, tf_idfNB, reviews, \u001b[0m\n",
       "\u001b[32mlabels\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: ... def classify_review\u001b[0m\u001b[32m(\u001b[0m\u001b[32mclean_review, tf_idfSVM, tf_idfNB\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: ... ... train_model\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtf_idfSVM, tf_idfNB, \u001b[0m\n",
       "\u001b[32mreviews, labels\u001b[0m\u001b[32m)\u001b[0m\u001b[32m cleaned_review = get_clean_review\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m... L Generated solution \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdetails omitted\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Language \u001b[0m\n",
       "\u001b[32mModel Figure 1: Overview of OSS-INSTRUCT and the pass@1 results of different LLMs on HumanEval \u001b[0m\u001b[32m(\u001b[0m\u001b[32m+\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32marXiv:2312.02120v1 \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.CL\u001b[0m\u001b[32m]\u001b[0m\u001b[32m 4 Dec 2023 1 Introduction Code generation, also known as program synthesis \u001b[0m\u001b[32m[\u001b[0m\u001b[32mGulwani\u001b[0m\n",
       "\u001b[32met al., 2017\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, is a long-standing challenge in computer science. In the past few decades, a large body of \u001b[0m\n",
       "\u001b[32mresearch has been studying symbolic approaches, such as abstraction-based synthesis \u001b[0m\u001b[32m[\u001b[0m\u001b[32mWang et al., 2017, Feng \u001b[0m\n",
       "\u001b[32met al., 2018\u001b[0m\u001b[32m]\u001b[0m\u001b[32m for'\u001b[0m,\n",
       "    \u001b[32m'keywords'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'clean_review'\u001b[0m, \u001b[32m'learn_model'\u001b[0m, \u001b[32m'tf_idfsvm'\u001b[0m, \u001b[32m'sklearn'\u001b[0m, \u001b[32m'generated'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(keys[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document\n",
    "goodDocs = []\n",
    "for i in range(0,len(keys)):\n",
    "  goodDocs.append(Document(page_content = keys[i]['doc'],\n",
    "                          metadata = {'source': keys[i]['document'],\n",
    "                              'type': 'chunk',\n",
    "                              'title': keys[i]['title'],\n",
    "                              'author': keys[i]['author'],\n",
    "                              'url' : keys[i]['url'],\n",
    "                              'keywords' : keys[i]['keywords']\n",
    "                              }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">' (details omitted) DPosNeg.py DLog.cpp DGrantInfo.ts DProgram.cs DStrength.swift D… </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Open-source codebase learn_model( tf_idfSVM, tf_idfNB, target) def get_clean_review(raw_review): letters_only </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">= re.sub( \"[^a-zA-Z]\", \" \", raw_review) Seed code snippet OSS-INSTRUCT Please gain inspiration from the code </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">snippet to create a highquality programming problem… Prompt (details omitted) from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sklearn.feature_extraction.text import TfidfVectorizer ... def get_clean_review(raw_review): ... def </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">train_model(tf_idfSVM, tf_idfNB, reviews, labels): ... def classify_review(clean_review, tf_idfSVM, tf_idfNB):</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">... ... train_model(tf_idfSVM, tf_idfNB, reviews, labels) cleaned_review = get_clean_review(...)... L </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Generated solution (details omitted) Language Model Figure 1: Overview of OSS-INSTRUCT and the pass@1 results </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of different LLMs on HumanEval (+) arXiv:2312.02120v1 [cs.CL] 4 Dec 2023 1 Introduction Code generation, also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">known as program synthesis [Gulwani et al., 2017], is a long-standing challenge in computer science. In the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">past few decades, a large body of research has been studying symbolic approaches, such as abstraction-based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">synthesis [Wang et al., 2017, Feng et al., 2018] for'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-03 18.41.12 Governing societies with Artificial Intelligence.txt'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'chunk'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Magicoder: Source Code Is All You Need'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://arxiv.org/pdf/2312.02120v1.pdf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'clean_review'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'learn_model'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tf_idfsvm'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sklearn'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'generated'</span><span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdetails omitted\u001b[0m\u001b[32m)\u001b[0m\u001b[32m DPosNeg.py DLog.cpp DGrantInfo.ts DProgram.cs DStrength.swift D… \u001b[0m\n",
       "\u001b[32mOpen-source codebase learn_model\u001b[0m\u001b[32m(\u001b[0m\u001b[32m tf_idfSVM, tf_idfNB, target\u001b[0m\u001b[32m)\u001b[0m\u001b[32m def get_clean_review\u001b[0m\u001b[32m(\u001b[0m\u001b[32mraw_review\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: letters_only \u001b[0m\n",
       "\u001b[32m= re.sub\u001b[0m\u001b[32m(\u001b[0m\u001b[32m \"\u001b[0m\u001b[32m[\u001b[0m\u001b[32m^a-zA-Z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\", \" \", raw_review\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Seed code snippet OSS-INSTRUCT Please gain inspiration from the code \u001b[0m\n",
       "\u001b[32msnippet to create a highquality programming problem… Prompt \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdetails omitted\u001b[0m\u001b[32m)\u001b[0m\u001b[32m from \u001b[0m\n",
       "\u001b[32msklearn.feature_extraction.text import TfidfVectorizer ... def get_clean_review\u001b[0m\u001b[32m(\u001b[0m\u001b[32mraw_review\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: ... def \u001b[0m\n",
       "\u001b[32mtrain_model\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtf_idfSVM, tf_idfNB, reviews, labels\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: ... def classify_review\u001b[0m\u001b[32m(\u001b[0m\u001b[32mclean_review, tf_idfSVM, tf_idfNB\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\u001b[0m\n",
       "\u001b[32m... ... train_model\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtf_idfSVM, tf_idfNB, reviews, labels\u001b[0m\u001b[32m)\u001b[0m\u001b[32m cleaned_review = get_clean_review\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m... L \u001b[0m\n",
       "\u001b[32mGenerated solution \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdetails omitted\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Language Model Figure 1: Overview of OSS-INSTRUCT and the pass@1 results \u001b[0m\n",
       "\u001b[32mof different LLMs on HumanEval \u001b[0m\u001b[32m(\u001b[0m\u001b[32m+\u001b[0m\u001b[32m)\u001b[0m\u001b[32m arXiv:2312.02120v1 \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.CL\u001b[0m\u001b[32m]\u001b[0m\u001b[32m 4 Dec 2023 1 Introduction Code generation, also \u001b[0m\n",
       "\u001b[32mknown as program synthesis \u001b[0m\u001b[32m[\u001b[0m\u001b[32mGulwani et al., 2017\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, is a long-standing challenge in computer science. In the \u001b[0m\n",
       "\u001b[32mpast few decades, a large body of research has been studying symbolic approaches, such as abstraction-based \u001b[0m\n",
       "\u001b[32msynthesis \u001b[0m\u001b[32m[\u001b[0m\u001b[32mWang et al., 2017, Feng et al., 2018\u001b[0m\u001b[32m]\u001b[0m\u001b[32m for'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'source'\u001b[0m: \u001b[32m'2023-12-03 18.41.12 Governing societies with Artificial Intelligence.txt'\u001b[0m,\n",
       "        \u001b[32m'type'\u001b[0m: \u001b[32m'chunk'\u001b[0m,\n",
       "        \u001b[32m'title'\u001b[0m: \u001b[32m'Magicoder: Source Code Is All You Need'\u001b[0m,\n",
       "        \u001b[32m'author'\u001b[0m: \u001b[32m'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'\u001b[0m,\n",
       "        \u001b[32m'url'\u001b[0m: \u001b[32m'https://arxiv.org/pdf/2312.02120v1.pdf'\u001b[0m,\n",
       "        \u001b[32m'keywords'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'clean_review'\u001b[0m, \u001b[32m'learn_model'\u001b[0m, \u001b[32m'tf_idfsvm'\u001b[0m, \u001b[32m'sklearn'\u001b[0m, \u001b[32m'generated'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(goodDocs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
